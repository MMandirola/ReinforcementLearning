{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLeDXUNk0zCH"
   },
   "source": [
    "# Pytorch\n",
    "Marcelo Mandirola\n",
    "https://pytorch.org/\n",
    "https://pytorch.org/tutorials/\n",
    "https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "Pytorch es un framework de machine learning que nos permite rápidamente diseñar, entrenar y testear modelos de machine learning (en particular, redes neuronales). \n",
    "\n",
    "Vamos a utilizar este framework para implementar el obligatorio del curso, por eso, en la clase de hoy vamos a ver una breve introduccion al framework y las redes neuronales. Vamos a prestar detallada atencion a dos tipos de modelos: las redes FeedForward (neuronas que se conectan entre sí en una modalidad de \"cascada secuencial\").\n",
    "\n",
    "Este notebook debería servir como base para implementar todas las operaciones necesarias para resolver el obligatorio, así como tambien cualquier otra tarea básica de Deep Learning.\n",
    "\n",
    "### A Entregar:\n",
    "\n",
    "- Este mismo notebook con la solucion a todos los problemas planteados. Pueden trabajar en grupos de hasta 3 estudiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocrqKXBzohxf"
   },
   "source": [
    "### Creación de tensores.\n",
    "Los tensores pueden crearse con listas o numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "labMfKXmoh_N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.],\n",
       "        [ 1., -1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1., -1.], [1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3sjdOL4oiBf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBCKc9UDoiDk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([2, 4], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de tensores.\n",
    "Los tensores pueden accederse mediante las directivas de slicing y e indexación de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([[1, 8, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x[1][2])\n",
    "x[0][1] = 8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones sobre tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor(2)\n",
    "z = torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.0000, 1.5000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1966])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.mv(z, x)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2964, -0.0471,  0.1307,  0.0922],\n",
       "        [ 1.4925,  0.0478, -1.3756, -0.4495]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 4)\n",
    "r = torch.mm(mat1, mat2)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(w.size())                      \n",
    "print(torch.numel(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing (reshaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x: torch.Size([2, 3])\n",
      "Size of y: torch.Size([6])\n",
      "Size of z: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)   \n",
    "print('Size of x:', x.size())\n",
    "y = x.view(6) \n",
    "print('Size of y:', y.size())\n",
    "z = x.view(-1, 2) \n",
    "print('Size of z:', z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de gradientes\n",
    "Pytorch habilita al cálculo automático de gradientes (autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(1., grad_fn=<MeanBackward0>)\n",
      "tensor([[ 0.5000, -0.5000],\n",
      "        [ 0.5000,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "print(x.grad)\n",
    "out = x.pow(2).mean()\n",
    "print(out)\n",
    "out.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hur5OGlgj-Ct"
   },
   "source": [
    "## Uso automático de GPU\n",
    "\n",
    "En Colab tenemos 12 Horas de GPU gratis para usar (cambiando el runtime type), esto nos permite entrenar modelos de DL mucho mas rápido. La celda de código abajo detecta si tenemos una GPU disponible o no y nos va a permitir escribir código genérico para cualquier dispositivo.\n",
    "\n",
    "***\n",
    "Recomendamos fuertemente utilizar CPU lo más posible mientras probamos código y usar la GPU solo para cuando sabemos que todo funciona y queremos obtener resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpnJwoJPjiOE",
    "outputId": "e21f35b0-4b0b-43ef-80e4-c842418cb66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbvuXJQAosTl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time:\n",
      "112 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      " \n",
      "GPU time:\n",
      "6.33 ms ± 61.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 900000).cpu()            # Initialize with random number (uniform distribution)\n",
    "y = torch.randn(900000,200).cpu()           # With normal distribution (SD=1, mean=0)\n",
    "z = torch.randperm(200).cpu()           # Size 200. Random permutation of integers from 0 to 200\n",
    "\n",
    "print('CPU time:')\n",
    "%timeit torch.mm(x,y)+z\n",
    "\n",
    "x = torch.rand(2, 900000).cuda()            \n",
    "y = torch.randn(900000,200).cuda()          \n",
    "z = torch.randperm(200).to(DEVICE)  # Manda al tensor al dispositivo que le pasamos (en este caso cuda:0)\n",
    "\n",
    "print(' ')\n",
    "print('GPU time:')\n",
    "%timeit torch.mm(x,y)+z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nl8qO_Sf2WZ1"
   },
   "source": [
    "## FeedForward networks\n",
    "\n",
    "Son la unidad más simple de red neuronal, con su origen en el perceptron de muchas capas. La idea es crear una secuencia lineal de neuronas (capa) que reciben nuestro input. \n",
    "\n",
    "![Image](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)\n",
    "\n",
    "De esta manera la primera capa de neuronas (input layer) recibe los datos y las capas subsiguientes reciben el resultados de capas anteriores. La última capa (output layer) es la encargada de generar una predicción a partir de nuestros inputs.\n",
    "\n",
    "***\n",
    "\n",
    "En este notebook vamos a usar un dataset muy simple y conocido de imágenes, Fashion-MNIST. Se trata de un dataset de ropa y calzado, la idea es usar redes neuronales para clasificar cada una de las imágenes el tipo de ropa que representa. \n",
    "\n",
    "Para trabajar con imagenes vamos a hacer uso de una librería complementaria a Pytorch: **torchvision** (https://pytorch.org/docs/stable/torchvision/index.html) que incluye varios datasets precargados, modelos preentrenados y algunas utilidades para trabajar con imágenes que nos van a resultar útiles.\n",
    "\n",
    "*** \n",
    "\n",
    "En la celda de abajo vamos a carga nuestro dataset y mostrar algunas imagenes de ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "JpVIgjO52Uou",
    "outputId": "1efde127-8277-401d-af9d-d7bfb3129a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset 60000 imagenes.\n",
      "Clases posibles: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "Objeto imagen: <PIL.Image.Image image mode=L size=28x28 at 0x7FF13D7C4700> - Clase 9\n",
      "Detalles de la imagen (28, 28) pixeles\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "mnist_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True)\n",
    "\n",
    "print(f\"Tamaño del dataset {len(mnist_dataset)} imagenes.\")\n",
    "print(f\"Clases posibles: {mnist_dataset.classes}\")\n",
    "\n",
    "data_idx = 0  # Indice (0-59999) de la imagen que queremos ver\n",
    "image, label = mnist_dataset[0] \n",
    "\n",
    "print(f\"Objeto imagen: {image} - Clase {label}\")\n",
    "print(f\"Detalles de la imagen {image.size} pixeles\")\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peWiTvQq2Uua"
   },
   "source": [
    "### Clasificador\n",
    "\n",
    "Ahora que tenemos una idea de como es nuestro dataset, vamos a crear un modelo FeedForward para predecir la clase de la imagen que usemos como input. \n",
    "\n",
    "Antes que nada, vamos a necesitar dividir el dataset total en conjuntos de **entrenamiento**, **validacion** y **test**. Vamos a usar un ratio de 80 y 20% respectivamente. El set de test se puede descargar por separado con torchvision. Además, vamos a necesitar una manera de cargar **batches** de datos a la vez, para entrenar nuestra red. Pytorch nos proporciona varias ayudas para esto.\n",
    "\n",
    "***\n",
    "\n",
    "Finalmente, queda aclarar el uso de **tranformaciones** sobre las imágenes. Por lo pronto, tenemos objetos de tipo PIL Image, necesitamos (al menos) convertirlos en Tensores, para que Pytorch los pueda manejar.\n",
    "\n",
    "Hay un numero inmenso de transformaciones posibles que podemos usar en nustras imagenes, en este caso basta con tranformarlas a tensores, pero dejamos este link para otros casos: https://pytorch.org/docs/stable/torchvision/transforms.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto nos permite cambiarle la forma a un tensor aplicandole una transformacion. \n",
    "\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return torch.reshape(img, self.new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVBOfbYX2UzC"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_transforms = transforms.Compose([transforms.ToTensor(), ReshapeTransform((-1,))])\n",
    "\n",
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=img_transforms)\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=img_transforms)\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKgtfpeArQPd"
   },
   "source": [
    "### Modelo\n",
    "\n",
    "Vamos a considerar cada imagen como un tensor de una sola dimensión, de largo 28*28 = 784. Cada uno de esos valores representa el valor de un pixel de nuestra imagen original.\n",
    "\n",
    "Nuestra red va a recibir ese tensor como input (en realidad, un batch de tensores de largo 784) que va a ser trabajado por varias capas ocultas con diferente número de neuronas hasta llegar a una capa de salida con 10 outputs, 1 por cada clase posible.\n",
    "\n",
    "***\n",
    "\n",
    "Vamos utilizar capas conectadas totalmente, tambien conocidas como Fully Connected, Dense, o Linear en Pytorch (https://pytorch.org/docs/stable/nn.html). Para crearlas necesitamos especificar las dimensiones del tensor de entrada, y el de salida; luego internamente Pytorch genera la matriz de pesos por los cuales multiplicar la entrada para generar la salida. Luego de cada una de estas operaciones necesitamos usar una funcion de activacion no linear, en este caso, vamos a usar ReLU: https://pytorch.org/docs/stable/nn.html#relu. \n",
    "\n",
    "***\n",
    "\n",
    "Para implementar un modelo **cualquiera** alcanza con definir un metodo **init** donde especificamos la arquitectura del mismo, y un método **forward** donde especificamos cómo interactúan nuestras capas frente a un nuevo input.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "h-SZey-Qqj1P",
    "outputId": "c3466114-d313-4b5c-c589-e6027ac4fe6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardModel(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definicion del modelo que vamos a usar. En Pytorch los modelos se definen como clases, que heredan de nn.Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, number_classes=10):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=128)\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.output = nn.Linear(in_features=64, out_features=number_classes)\n",
    "  \n",
    "    def forward(self, new_input):\n",
    "        result = F.relu(self.linear1(new_input))\n",
    "        result = F.relu(self.linear2(result))\n",
    "        logits = self.output(result)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = FeedForwardModel(number_classes=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYv_kROa0o54"
   },
   "source": [
    "### Entrenando el modelo\n",
    "\n",
    "Para entrenar un modelo necesitamos una funcion de costo o pérdida (normalmente referida como loss function: https://pytorch.org/docs/stable/nn.html#loss-functions). En este curso no nos vamos a meter en mucho detalle sobre las funciones de costo, para este ejercicio y el siguiente vamos a usar la CrossEntropyLoss, y cuando necesiten otra la vamos a especificar.\n",
    "\n",
    "El objetivo de esta funcion es darnos un valor de que tan malas fueron las predicciones del modelo respecto a los valores de verdad. Haciendo uso de backpropagation y del gradiente de esta funcion podemos optimizar los pesos de nuestra red tal que \"aprenda\" a hacer mejores predicciones. De nuevo, la lógica detras de toda esta optimización no nos compete en este curso y lo dejamos para la disciplina de Deep Learning.\n",
    "\n",
    "***\n",
    "Como mencionamos arriba, el costo de computa usando las predicciones del modelo y las etiquetas verdaderas de nuestros datos y, el trabajo de actualizar los pesos usando los gradientes lo realiza un optimizador de Pytorch: https://pytorch.org/docs/stable/optim.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYtCjd9cqj3x"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "ff_model = FeedForwardModel(number_classes=10).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "ff_optimizer = optim.SGD(ff_model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "zBEV-LNsqj6o",
    "outputId": "7965009c-f8c9-420c-c3b0-0a290420156b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_func, optimizer, epochs):\n",
    "    for epoch in range(epochs):  # Iteramos sobre el dataset entero muchas veces\n",
    "\n",
    "        running_loss = 0.0  \n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Nuestros datos son imagenes y la clase de cada una.\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Reseteamos los gradientes de los pesos del modelo.\n",
    "            optimizer.zero_grad()   \n",
    "\n",
    "            # Obtenemos las predicciones para las nuevas imagenes llamando a nuestro modelo.\n",
    "            predictions = model(images)    \n",
    "\n",
    "            # Calulamos el costo de nuestras predicciones respecto a la verdad\n",
    "            loss = loss_func(predictions, labels)\n",
    "\n",
    "            # Computamos los gradientes con backward y actualizamos los pesos con un optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Estadísiticas\n",
    "            running_loss += loss.item()\n",
    "            if i % 500 == 499:    # Imprimimos luego de 1000 batches de datos\n",
    "                print(f\"Epoch: {epoch + 1}, Batch: {i + 1} - Loss: {running_loss / 500:.5f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        # Luego de cada epoch de entrenamiento vemos la performance (accuracy) en el set de validacion\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0.0\n",
    "\n",
    "            for i, data in enumerate(val_loader):\n",
    "                images, labels = data\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                predictions = model(images)\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "                correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "        print(f\"Validation accuracy {(100 * correct_predictions / len(val_loader.dataset)):.2f} %\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "leKy45f4N0q8",
    "outputId": "24580975-aa42-4f23-9f2c-3f76e9a25075"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    # Finalmente Reportamos la performance en el test set:\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            predictions = model(images)\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "    print(f\"Test set accuracy {(100 * correct_predictions / len(test_loader.dataset)):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 1.25126\n",
      "Validation accuracy 77.48 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.59047\n",
      "Validation accuracy 82.27 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.49796\n",
      "Validation accuracy 82.99 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.46282\n",
      "Validation accuracy 83.51 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.43237\n",
      "Validation accuracy 84.35 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.41490\n",
      "Validation accuracy 85.17 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.39828\n",
      "Validation accuracy 85.62 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.38105\n",
      "Validation accuracy 85.74 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.36834\n",
      "Validation accuracy 85.92 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.35873\n",
      "Validation accuracy 86.82 %\n",
      "Epoch: 11, Batch: 500 - Loss: 0.34663\n",
      "Validation accuracy 86.62 %\n",
      "Epoch: 12, Batch: 500 - Loss: 0.33702\n",
      "Validation accuracy 86.81 %\n",
      "Epoch: 13, Batch: 500 - Loss: 0.32922\n",
      "Validation accuracy 87.33 %\n",
      "Epoch: 14, Batch: 500 - Loss: 0.32083\n",
      "Validation accuracy 87.33 %\n",
      "Epoch: 15, Batch: 500 - Loss: 0.31455\n",
      "Validation accuracy 87.64 %\n",
      "Test set accuracy 87.02 %\n"
     ]
    }
   ],
   "source": [
    "# Usando las funciones definidas arriba entrenar un modelo es trivial\n",
    "\n",
    "ff_model = train_model(ff_model, train_loader, val_loader, loss_func=criterion, optimizer=ff_optimizer, epochs=15)\n",
    "test_model(ff_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 1\n",
    "\n",
    "Cree y entrene un modelo de red FeedForward que funcione mejor que el visto en clase. Puede usar lo que considere necesario (siempre dentro del mundo de redes feed forward - nada de convoluciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardModel(\n",
       "  (linear1): Linear(in_features=784, out_features=356, bias=True)\n",
       "  (linear2): Linear(in_features=356, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, number_classes=10):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=356)\n",
    "        self.linear2 = nn.Linear(in_features=356, out_features=64)\n",
    "        self.output = nn.Linear(in_features=64, out_features=number_classes)\n",
    "  \n",
    "    def forward(self, new_input):\n",
    "        result = F.relu(self.linear1(new_input))\n",
    "        result = F.relu(self.linear2(result))\n",
    "        logits = self.output(result)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = FeedForwardModel(number_classes=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "ff_model2 = FeedForwardModel(number_classes=10).to(DEVICE)\n",
    "criterion2 = nn.CrossEntropyLoss().to(DEVICE)\n",
    "ff_optimizer2 = optim.Adam(ff_model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 0.61473\n",
      "Validation accuracy 83.88 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.39317\n",
      "Validation accuracy 86.70 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.34596\n",
      "Validation accuracy 87.08 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.32058\n",
      "Validation accuracy 86.97 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.29695\n",
      "Validation accuracy 86.90 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.28268\n",
      "Validation accuracy 87.92 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.27242\n",
      "Validation accuracy 88.18 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.25558\n",
      "Validation accuracy 87.91 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.24554\n",
      "Validation accuracy 87.59 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.23499\n",
      "Validation accuracy 88.33 %\n",
      "Epoch: 11, Batch: 500 - Loss: 0.22679\n",
      "Validation accuracy 88.51 %\n",
      "Epoch: 12, Batch: 500 - Loss: 0.21814\n",
      "Validation accuracy 89.15 %\n",
      "Epoch: 13, Batch: 500 - Loss: 0.21066\n",
      "Validation accuracy 88.89 %\n",
      "Epoch: 14, Batch: 500 - Loss: 0.20335\n",
      "Validation accuracy 88.85 %\n",
      "Epoch: 15, Batch: 500 - Loss: 0.19487\n",
      "Validation accuracy 89.08 %\n",
      "Epoch: 16, Batch: 500 - Loss: 0.18858\n",
      "Validation accuracy 89.10 %\n",
      "Epoch: 17, Batch: 500 - Loss: 0.18053\n",
      "Validation accuracy 89.39 %\n",
      "Epoch: 18, Batch: 500 - Loss: 0.18034\n",
      "Validation accuracy 89.18 %\n",
      "Epoch: 19, Batch: 500 - Loss: 0.17027\n",
      "Validation accuracy 88.73 %\n",
      "Epoch: 20, Batch: 500 - Loss: 0.17086\n",
      "Validation accuracy 89.11 %\n",
      "Epoch: 21, Batch: 500 - Loss: 0.15841\n",
      "Validation accuracy 89.30 %\n",
      "Epoch: 22, Batch: 500 - Loss: 0.15461\n",
      "Validation accuracy 89.63 %\n",
      "Epoch: 23, Batch: 500 - Loss: 0.15064\n",
      "Validation accuracy 89.23 %\n",
      "Epoch: 24, Batch: 500 - Loss: 0.14527\n",
      "Validation accuracy 89.17 %\n",
      "Epoch: 25, Batch: 500 - Loss: 0.13857\n",
      "Validation accuracy 89.00 %\n",
      "Epoch: 26, Batch: 500 - Loss: 0.13427\n",
      "Validation accuracy 89.15 %\n",
      "Epoch: 27, Batch: 500 - Loss: 0.13261\n",
      "Validation accuracy 89.21 %\n",
      "Epoch: 28, Batch: 500 - Loss: 0.13007\n",
      "Validation accuracy 89.29 %\n",
      "Epoch: 29, Batch: 500 - Loss: 0.12259\n",
      "Validation accuracy 89.28 %\n",
      "Epoch: 30, Batch: 500 - Loss: 0.11955\n",
      "Validation accuracy 89.53 %\n",
      "Epoch: 31, Batch: 500 - Loss: 0.11467\n",
      "Validation accuracy 88.87 %\n",
      "Epoch: 32, Batch: 500 - Loss: 0.11358\n",
      "Validation accuracy 88.96 %\n",
      "Epoch: 33, Batch: 500 - Loss: 0.10844\n",
      "Validation accuracy 89.07 %\n",
      "Epoch: 34, Batch: 500 - Loss: 0.10381\n",
      "Validation accuracy 89.16 %\n",
      "Epoch: 35, Batch: 500 - Loss: 0.10544\n",
      "Validation accuracy 89.13 %\n",
      "Epoch: 36, Batch: 500 - Loss: 0.10245\n",
      "Validation accuracy 89.01 %\n",
      "Epoch: 37, Batch: 500 - Loss: 0.09035\n",
      "Validation accuracy 89.22 %\n",
      "Epoch: 38, Batch: 500 - Loss: 0.09888\n",
      "Validation accuracy 88.71 %\n",
      "Epoch: 39, Batch: 500 - Loss: 0.09525\n",
      "Validation accuracy 89.27 %\n",
      "Epoch: 40, Batch: 500 - Loss: 0.08866\n",
      "Validation accuracy 88.83 %\n",
      "Epoch: 41, Batch: 500 - Loss: 0.09203\n",
      "Validation accuracy 89.39 %\n",
      "Epoch: 42, Batch: 500 - Loss: 0.09091\n",
      "Validation accuracy 89.28 %\n",
      "Epoch: 43, Batch: 500 - Loss: 0.08735\n",
      "Validation accuracy 89.50 %\n",
      "Epoch: 44, Batch: 500 - Loss: 0.08159\n",
      "Validation accuracy 89.18 %\n",
      "Epoch: 45, Batch: 500 - Loss: 0.08379\n",
      "Validation accuracy 89.58 %\n",
      "Test set accuracy 89.04 %\n"
     ]
    }
   ],
   "source": [
    "ff_model = train_model(ff_model2, train_loader, val_loader, loss_func=criterion2, optimizer=ff_optimizer2, epochs=45)\n",
    "test_model(ff_model2, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Convolucionales\n",
    "\n",
    "\n",
    "![Image](https://www.unite.ai/wp-content/uploads/2019/12/Typical_cnn-1.png)\n",
    "\n",
    "\n",
    "Las redes convolucionales (CNNs) se basan en el uso de una tecnica muy usada en el campo de computer vision tradicional, las **convoluciones** (https://en.wikipedia.org/wiki/Kernel_(image_processing), la idea es crear un filtro pequeño que pasamos por encima de toda la imagen y nos permite detectar distintos elementos (como son líneas verticales, horizontales, diagonales, circulos, etc). EL gran problema de las convoluciones es que para crear dichos filtros debemos poder especificar distintos valores (pesos) para cada región en el mismo. \n",
    "\n",
    "Cada filtro (tambien conocido como kernel) nos permite identificar algo en particular en la imágen, y aplicar un filtro al resultado de otro (u otros) nos permite obtener informacion de más alto nivel (como por ejemplo detectar ojos, ruedas, puertas, etc).\n",
    "\n",
    "![Image](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "***\n",
    "\n",
    "Las redes convolucionales nos dan una manera de no sólo aprender los vaores óptimos para dichos filtros (mediante backprop) sino tambien la posibilidad de hacerlo a escala usndo un número arbitrario de los mismos. Una gran ventaja que nos trae el uso de filtros, es el hecho de que requieren de un número muy chico de pesos a entrenar, lo que reduce el tamaño de nuestra red y nos permite entrenar mas rápido (o redes mas grandes y profundas con el mismo hardware).\n",
    "\n",
    "Una cosa a notar en las redes convolucionales es el hecho de que las imágenes se van reduciendo en su tamaño a medida que fluyen por la red, esto se debe a la opeación de `maxpooling` que toma regiones (por lo general de 2x2) en nuestra imagen y se queda con el valor más alto en la zona, reduciendo asi el tamaño de la imagen. El resultado de aplicar un filtro de convolución a una imagen se llama `feature_map` y se puede pensar como otra imagen que describe la características de la original. \n",
    "\n",
    "***\n",
    "\n",
    "Al final de nuestra red, necesitamos formar una predicción de la clase de nuestra imagen, por lo que tenemos que \"achatar\" estos feature maps y pasarlos por una (o varias) capas lineales que generen una predicción. Esto se puede ver como representar toda la informacion que conocemos de la imagen, como por ejemplo si tiene nariz, orejas, pelo, en un sólo vector; y decidir que ese vector representa a un perro.\n",
    "\n",
    "***\n",
    "\n",
    "Para empezar, volvemos a definir nuestros conjuntos de datos. Esta vez, sin hacer uso de ninguna transformacion sobre la imagen (mas que transformarla en un tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Convolucional\n",
    "\n",
    "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arcquitectura y comportamiento de los componentes del modelo. \n",
    "\n",
    "En particular vamos a usar:\n",
    "\n",
    "- capas convolucionales de 2D (https://pytorch.org/docs/stable/nn.html#conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes). \n",
    "\n",
    "- Capas de maxpooling (https://pytorch.org/docs/stable/nn.html#maxpool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
    "\n",
    "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self, number_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
    "        \n",
    "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
    "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
    "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=7*7*8, out_features=128) \n",
    "        \n",
    "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, new_input):\n",
    "        result = self.conv1(new_input)\n",
    "        result = F.relu(self.pooling_layer(result))\n",
    "        \n",
    "        result = self.conv2(result)\n",
    "        result = F.relu(self.pooling_layer(result))     \n",
    "        \n",
    "        # \"Achatamos\" los feature maps\n",
    "        result = result.reshape((-1, self.fc1.in_features))\n",
    "        result = F.relu(self.fc1(result))\n",
    "        \n",
    "        return self.output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 0.71262\n",
      "Validation accuracy 86.76 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.36003\n",
      "Validation accuracy 88.59 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.30695\n",
      "Validation accuracy 89.46 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.28661\n",
      "Validation accuracy 89.63 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.25971\n",
      "Validation accuracy 89.16 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.24461\n",
      "Validation accuracy 90.12 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.23173\n",
      "Validation accuracy 90.65 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.22182\n",
      "Validation accuracy 90.29 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.20944\n",
      "Validation accuracy 90.25 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.20224\n",
      "Validation accuracy 90.26 %\n",
      "Test set accuracy 89.29 %\n"
     ]
    }
   ],
   "source": [
    "conv_model = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "conv_model = train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=10)\n",
    "test_model(conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2\n",
    "\n",
    "Cree y entrene un modelo de red Convolucional que funcione mejor que el visto en clase. Puede usar lo que considere necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self, number_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
    "        \n",
    "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
    "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
    "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=7*7*16, out_features=128) \n",
    "        \n",
    "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, new_input):\n",
    "        result = self.conv1(new_input)\n",
    "        result = F.relu(self.pooling_layer(result))\n",
    "        \n",
    "        result = self.conv2(result)\n",
    "        result = F.relu(self.pooling_layer(result))     \n",
    "        \n",
    "        # \"Achatamos\" los feature maps\n",
    "        result = result.reshape((-1, self.fc1.in_features))\n",
    "        result = F.relu(self.fc1(result))\n",
    "        \n",
    "        return self.output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 0.65492\n",
      "Validation accuracy 86.97 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.33917\n",
      "Validation accuracy 85.97 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.28490\n",
      "Validation accuracy 90.12 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.25392\n",
      "Validation accuracy 90.86 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.23200\n",
      "Validation accuracy 90.70 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.21547\n",
      "Validation accuracy 91.39 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.20433\n",
      "Validation accuracy 91.04 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.19128\n",
      "Validation accuracy 90.67 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.17256\n",
      "Validation accuracy 91.27 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.16113\n",
      "Validation accuracy 91.17 %\n",
      "Test set accuracy 90.30 %\n"
     ]
    }
   ],
   "source": [
    "conv_model2 = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "\n",
    "criterion2 = nn.CrossEntropyLoss().to(DEVICE)\n",
    "conv_optimizer2 = optim.SGD(conv_model2.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "conv_model = train_model(conv_model2, train_loader, val_loader, loss_func=criterion2, optimizer=conv_optimizer2, epochs=10)\n",
    "test_model(conv_model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
